Задача 2.2
Вы проверили работоспособность нового ETL-инструмента (Spark) и теперь с его помощью необходимо разработать один из типовых процессов для банковского хранилища - построение зеркал.
Зеркало - это таблица в сыром или детальном слое, в которой содержится последнее (актуальное) состояние каждой еë сущности (записи/строки). То есть, если 3 раза запускался процесс загрузки, который выделял дельту и в каждой такой дельте была одна и так же сущность (какой-то один счëт), у которой менялись вторичные параметры - то в зеркале должна остаться одна строка с последним состоянием, каждого из вторичных параметров.
Для того что бы такой процесс был возможным - у таблицы обязательно должен быть уникальный ключ (id или номер счëта, например).
Данный механизм необходимо сделать универсальным - в него должны передаваться:  путь где хранятся дельты, наименование таблицы и список полей, являющимся уникальным ключём. Согласовано, что дельты внутри указанного пути (основной директории) всегда хранятся в виде вложенных директорий с наименованием в порядке возрастания (например: 1000, 1001…..) – это тоже нужно заложить в универсальный механизм.
Для демонстрации работоспособности вашего приложения нужно будет обработать 4 дельты с изменениями некоторых параметров в справочнике счетов. Каждая дельта - это csv- файл внутри директории с наименованием id- дельты (id сессии загрузки).
Итоговый результат должен быть сохранëн в csv- файл в директории «mirr_md_account_d».
Важно добавить в этот механизм процесс логирования, что бы не обрабатывать повторно, ранее загруженные дельты - это будет экономить время и ресурсы сервера.
Логи могут быть в виде отдельной директории с файлами того формата, что вы сочтëте для себя удобным (csv / parquet / json) или в реляционной БД (во вложении файл со скриптами установки PostgresSQL в Ubuntu). В них должна содержатся информация с датой-время старта и завершения загрузки каждой из дельт (то есть запоминать еë id). Так же в логах должно сохраняться наименование обновляемой таблицы.В некотором банке внедрили новую frontend-систему для работы с клиентами, а так же обновили и саму базу данных. Большую часть данных успешно были перенесены из старых БД в одну новую централизованную БД.  Но в момент переключения со старой системы на новую возникли непредвиденные проблемы в ETL-процессе, небольшой период (конец 2017 начало 2018 года) так и остался в старой базе. Старую базу отключили, а не выгруженные данные сохранили в csv-файлы. Недавно банку потребовалось построить отчёт по 101 форме. Те данные что остались в csv-файлах тоже нужны. Загрузить их в новую БД не получиться из-за архитектурных и управленческих сложностей, нужно рассчитать витрину отдельно. Но для этого сначала нужно загрузить исходные данные из csv-файлов в детальный слой (DS) хранилища в PostgreSQL.
